{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "prompt_path = \"prompts/python_cot.txt\"\n",
    "dataset_path = \"data/MATH_DPO_COT\"\n",
    "checkpoint_path = \"checkpoint_kto_dataset_python_cot.yaml\"\n",
    "\n",
    "prompt = open(prompt_path, \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import yaml\n",
    "from trl import TextEnvironment, AutoModelForCausalLMWithValueHead\n",
    "from transformers import (\n",
    "    load_tool,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _exact_match_reward(responses, answers):\n",
    "    \"\"\"Reward if generated response contains correct answer.\"\"\"\n",
    "    rewards = []\n",
    "    for response, answer in zip(responses, answers):\n",
    "        reward = 0.0\n",
    "        predicted_number = _get_answer(response)\n",
    "        if predicted_number is not None:\n",
    "            if np.abs(predicted_number - float(answer)) < 0.1:\n",
    "                reward += 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def _get_answer(response):\n",
    "    try:\n",
    "        pattern = r\"Result\\s*=\\s*(-?\\d+(?:\\.\\d+)?)\\s*<submit>\"\n",
    "        match_pattern = re.findall(pattern, response)\n",
    "        if match_pattern:\n",
    "            return float(match_pattern[0])\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb5e4f0f2f742e5b568b0cd992aa80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Meta-Llama-3-8B-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're loading a tool from the Hub from None. Please make sure this is a source that you trust as the code within that tool will be executed on your machine. Always verify the code of the tools that you load. We recommend specifying a `revision` to ensure you're loading the code that you have checked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-18 16:52:30,592] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_name, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "env = TextEnvironment(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [load_tool(\"lvwerra/python-interpreter\")],\n",
    "    _exact_match_reward,\n",
    "    prompt,\n",
    "    generation_kwargs={\"max_new_tokens\": 512, \"pad_token_id\": tokenizer.eos_token_id},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6137ca6e058d4e99b9fb08c1f869aa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc78b883d0f4414f9e334282a7569262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_dir=\"data/MATH\")\n",
    "\n",
    "\n",
    "def is_real_number(text):\n",
    "    try:\n",
    "        float(text)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_answer(text):\n",
    "    try:\n",
    "        match = re.search(r\"\\\\boxed{(.+?)}\", text)\n",
    "        return match.group(1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "dataset_with_answer = dataset.map(\n",
    "    lambda x: {\"problem\": x[\"problem\"], \"answer\": extract_answer(x[\"solution\"])}\n",
    ")\n",
    "dataset_with_answer = dataset_with_answer.filter(lambda x: is_real_number(x[\"answer\"]))\n",
    "dataset_with_answer = dataset_with_answer.filter(lambda x: len(x[\"problem\"]) < 500)\n",
    "dataset_with_answer = dataset_with_answer.rename_column(\"problem\", \"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953bc76319424a16b6b834aff2e672ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6c15135a7748a1b5fb90906bec3e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eafa568609c406cbf98ae9935cf8fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327aa4186dd441cb9e7fdf45beea2ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31da039a26144b2a5ad54a9d579f92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = []\n",
    "completions = []\n",
    "labels = []\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "start_index = 0\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    with open(checkpoint_path, \"r\") as f:\n",
    "        checkpoint = yaml.safe_load(f)\n",
    "        start_index = checkpoint.get(\"start_index\", 0)\n",
    "        start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "        prompts = checkpoint.get(\"prompts\", [])\n",
    "        completions = checkpoint.get(\"completions\", [])\n",
    "        labels = checkpoint.get(\"labels\", [])\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    for i in tqdm(range(start_index, len(dataset_with_answer[\"train\"]), batch_size)):\n",
    "        batch_rows = dataset_with_answer[\"train\"][i : i + batch_size]\n",
    "\n",
    "        queries_tensor, responses_tensor, masks, rewards, histories = env.run(\n",
    "            batch_rows[\"query\"], answers=batch_rows[\"answer\"]\n",
    "        )\n",
    "        responses = tokenizer.batch_decode(responses_tensor)\n",
    "        for query, response, reward in zip(batch_rows[\"query\"], responses, rewards):\n",
    "            prompts.append(prompt + query)\n",
    "            completions.append(response)\n",
    "            labels.append(True if reward > 0 else False)\n",
    "        checkpoint = {\n",
    "            \"start_index\": i + batch_size,\n",
    "            \"epoch\": epoch,\n",
    "            \"prompts\": prompts,\n",
    "            \"completions\": completions,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        with open(checkpoint_path, \"w\") as f:\n",
    "            yaml.safe_dump(checkpoint, f)\n",
    "\n",
    "new_dataset = Dataset.from_dict(\n",
    "    {\"prompt\": prompts, \"completion\": completions, \"label\": labels}\n",
    ")\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "new_dataset.save_to_disk(dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
