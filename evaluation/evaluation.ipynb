{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = 'prompts/costar_cot_1shot.txt'\n",
    "checkpoint_path = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "prompt = open(prompt_path, 'r').read()\n",
    "prompt = 'Submit your answer with the format: \"Result = 72 <submit>\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143bf962ba6b4610bc61bb153d49a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype='bfloat16',\n",
    ") \n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_texts: list[str]):\n",
    "    inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=False).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id)\n",
    "    output_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [output[len(input):] for input, output in zip(input_texts, output_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def _exact_match_reward(responses, answers):\n",
    "    \"\"\"Reward if generated response contains correct answer.\"\"\"\n",
    "    rewards = []\n",
    "    for response, answer in zip(responses, answers):\n",
    "        reward = 0.0\n",
    "        predicted_number = _get_answer(response)\n",
    "        if predicted_number is not None:\n",
    "            if np.abs(predicted_number - float(answer)) < 0.1:\n",
    "                reward += 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "def _get_answer(response):\n",
    "    try:\n",
    "        pattern = r\"Result\\s*=\\s*(-?\\d+(?:\\.\\d+)?)\\s*<submit>\"\n",
    "        match_pattern = re.findall(pattern, response)\n",
    "        if match_pattern:\n",
    "            return float(match_pattern[0])\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MATH_test_dataset():\n",
    "    dataset = load_dataset(\"json\", data_dir=\"data/MATH\")\n",
    "    def is_real_number(text):\n",
    "        try:\n",
    "            float(text)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    def extract_answer(text):\n",
    "        try:\n",
    "            match = re.search(r\"\\\\boxed{(.+?)}\", text)\n",
    "            return match.group(1)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    dataset_with_answer = dataset.map(lambda x: {\"problem\": x[\"problem\"], \"answer\": extract_answer(x[\"solution\"])})\n",
    "    dataset_with_answer = dataset_with_answer.filter(lambda x: is_real_number(x[\"answer\"]))\n",
    "    dataset_with_answer = dataset_with_answer.filter(lambda x: len(x['problem']) < 500)\n",
    "    dataset_with_answer = dataset_with_answer.rename_column(\"problem\", \"query\")\n",
    "    return dataset_with_answer['test']\n",
    "\n",
    "def get_aimo_test_dataset():\n",
    "    test_dataset = Dataset.from_csv(\"data/val.csv\")\n",
    "    test_dataset = test_dataset.rename_column(\"problem\", \"query\")\n",
    "    test_dataset = test_dataset.remove_columns([\"id\"])\n",
    "    return test_dataset\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataset):\n",
    "    batch_size = len(test_dataset)\n",
    "    responses = []\n",
    "\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i:i + batch_size]\n",
    "        batch_queries = [prompt + row for row in batch]\n",
    "        batch_responses = generate(model, batch_queries)\n",
    "        responses.extend(batch_responses)\n",
    "        \n",
    "    answers = test_dataset['answer']\n",
    "    rewards = _exact_match_reward(responses, answers)\n",
    "    print(f\"Exact match reward: {np.mean(rewards)}\")\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb2715080f64bc8a224c35c21e79c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7283915a56ad4b79a8c33b7d7e68278d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b486b66f921482f81a14ca362e2ce3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "dataset = get_MATH_test_dataset()\n",
    "responses = evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the positive difference between $120\\%$ of 30 and $130\\%$ of 20?\n",
      "Predicted:  1: 72\n",
      "level 2: 72\n",
      "level 3: 72\n",
      "level 4: 72\n",
      "level 5: 72\n",
      "level 6: 72\n",
      "level 7: 72\n",
      "level 8: 72\n",
      "level 9: 72\n",
      "level 10: 72\n",
      "level 11: 72\n",
      "level 12: 72\n",
      "level 13: 72\n",
      "level 14: 72\n",
      "level 15: 72\n",
      "level 16: 72\n",
      "level 17: 72\n",
      "level 18: 72\n",
      "level 19: 72\n",
      "level 20: 72\n",
      "level 21: 72\n",
      "level 22: 72\n",
      "level 23: 72\n",
      "level 24: 72\n",
      "level 25: 72\n",
      "level 26: 72\n",
      "level 27: 72\n",
      "level 28: 72\n",
      "level 29: 72\n",
      "level 30: 72\n",
      "level 31: 72\n",
      "level 32: 72\n",
      "level 33: 72\n",
      "level 34: 72\n",
      "level 35: 72\n",
      "level 36: 72\n",
      "level 37: 72\n",
      "level 38: 72\n",
      "level 39: 72\n",
      "level 40: 72\n",
      "level 41: 72\n",
      "level 42: 72\n",
      "level 43: 72\n",
      "level 44: 72\n",
      "level 45: 72\n",
      "level 46: 72\n",
      "level 47: 72\n",
      "level 48: 72\n",
      "level 49: 72\n",
      "level 50: 72\n",
      "level 51: 72\n",
      "level 52: 72\n",
      "level 53: 72\n",
      "level 54: 72\n",
      "level 55: 72\n",
      "level 56: 72\n",
      "level 57: 72\n",
      "level 58: 72\n",
      "level 59: 72\n",
      "level 60: 72\n",
      "level 61: 72\n",
      "level 62: 72\n",
      "level 63: 72\n",
      "level 64: 72\n",
      "level 65: 72\n",
      "level 66: 72\n",
      "level 67: 72\n",
      "level 68: 72\n",
      "level 69: 72\n",
      "level 70: 72\n",
      "level 71: 72\n",
      "level 72: 72\n",
      "level 73: 72\n",
      "level \n",
      "Answer: 10\n"
     ]
    }
   ],
   "source": [
    "check_index = 1\n",
    "\n",
    "print(f\"Query: {dataset[check_index]['query']}\")\n",
    "print(f\"Predicted: {responses[check_index]}\")\n",
    "print(f\"Answer: {dataset[check_index]['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
