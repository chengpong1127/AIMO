{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prompts/costar_cot_1shot.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts/costar_cot_1shot.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubmit your answer with the format: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult = 72 <submit>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/home/ntuai/AIMO/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prompts/costar_cot_1shot.txt'"
     ]
    }
   ],
   "source": [
    "prompt_path = \"prompts/costar_cot_1shot.txt\"\n",
    "checkpoint_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "prompt = open(prompt_path, \"r\").read()\n",
    "prompt = 'Submit your answer with the format: \"Result = 72 <submit>\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143bf962ba6b4610bc61bb153d49a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_texts: list[str]):\n",
    "    inputs = tokenizer(\n",
    "        input_texts, return_tensors=\"pt\", padding=True, truncation=False\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    output_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [output[len(input) :] for input, output in zip(input_texts, output_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def _exact_match_reward(responses, answers):\n",
    "    \"\"\"Reward if generated response contains correct answer.\"\"\"\n",
    "    rewards = []\n",
    "    for response, answer in zip(responses, answers):\n",
    "        reward = 0.0\n",
    "        predicted_number = _get_answer(response)\n",
    "        if predicted_number is not None:\n",
    "            if np.abs(predicted_number - float(answer)) < 0.1:\n",
    "                reward += 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def _get_answer(response):\n",
    "    try:\n",
    "        pattern = r\"Result\\s*=\\s*(-?\\d+(?:\\.\\d+)?)\\s*<submit>\"\n",
    "        match_pattern = re.findall(pattern, response)\n",
    "        if match_pattern:\n",
    "            return float(match_pattern[0])\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MATH_test_dataset():\n",
    "    dataset = load_dataset(\"json\", data_dir=\"data/MATH\")\n",
    "\n",
    "    def is_real_number(text):\n",
    "        try:\n",
    "            float(text)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def extract_answer(text):\n",
    "        try:\n",
    "            match = re.search(r\"\\\\boxed{(.+?)}\", text)\n",
    "            return match.group(1)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    dataset_with_answer = dataset.map(\n",
    "        lambda x: {\"problem\": x[\"problem\"], \"answer\": extract_answer(x[\"solution\"])}\n",
    "    )\n",
    "    dataset_with_answer = dataset_with_answer.filter(\n",
    "        lambda x: is_real_number(x[\"answer\"])\n",
    "    )\n",
    "    dataset_with_answer = dataset_with_answer.filter(lambda x: len(x[\"problem\"]) < 500)\n",
    "    dataset_with_answer = dataset_with_answer.rename_column(\"problem\", \"query\")\n",
    "    return dataset_with_answer[\"test\"]\n",
    "\n",
    "\n",
    "def get_aimo_test_dataset():\n",
    "    test_dataset = Dataset.from_csv(\"data/val.csv\")\n",
    "    test_dataset = test_dataset.rename_column(\"problem\", \"query\")\n",
    "    test_dataset = test_dataset.remove_columns([\"id\"])\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataset):\n",
    "    batch_size = len(test_dataset)\n",
    "    responses = []\n",
    "\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i : i + batch_size]\n",
    "        batch_queries = [prompt + row for row in batch]\n",
    "        batch_responses = generate(model, batch_queries)\n",
    "        responses.extend(batch_responses)\n",
    "\n",
    "    answers = test_dataset[\"answer\"]\n",
    "    rewards = _exact_match_reward(responses, answers)\n",
    "    print(f\"Exact match reward: {np.mean(rewards)}\")\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb2715080f64bc8a224c35c21e79c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7283915a56ad4b79a8c33b7d7e68278d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b486b66f921482f81a14ca362e2ce3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "dataset = get_MATH_test_dataset()\n",
    "responses = evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the positive difference between $120\\%$ of 30 and $130\\%$ of 20?\n",
      "Predicted:  1: 72\n",
      "level 2: 72\n",
      "level 3: 72\n",
      "level 4: 72\n",
      "level 5: 72\n",
      "level 6: 72\n",
      "level 7: 72\n",
      "level 8: 72\n",
      "level 9: 72\n",
      "level 10: 72\n",
      "level 11: 72\n",
      "level 12: 72\n",
      "level 13: 72\n",
      "level 14: 72\n",
      "level 15: 72\n",
      "level 16: 72\n",
      "level 17: 72\n",
      "level 18: 72\n",
      "level 19: 72\n",
      "level 20: 72\n",
      "level 21: 72\n",
      "level 22: 72\n",
      "level 23: 72\n",
      "level 24: 72\n",
      "level 25: 72\n",
      "level 26: 72\n",
      "level 27: 72\n",
      "level 28: 72\n",
      "level 29: 72\n",
      "level 30: 72\n",
      "level 31: 72\n",
      "level 32: 72\n",
      "level 33: 72\n",
      "level 34: 72\n",
      "level 35: 72\n",
      "level 36: 72\n",
      "level 37: 72\n",
      "level 38: 72\n",
      "level 39: 72\n",
      "level 40: 72\n",
      "level 41: 72\n",
      "level 42: 72\n",
      "level 43: 72\n",
      "level 44: 72\n",
      "level 45: 72\n",
      "level 46: 72\n",
      "level 47: 72\n",
      "level 48: 72\n",
      "level 49: 72\n",
      "level 50: 72\n",
      "level 51: 72\n",
      "level 52: 72\n",
      "level 53: 72\n",
      "level 54: 72\n",
      "level 55: 72\n",
      "level 56: 72\n",
      "level 57: 72\n",
      "level 58: 72\n",
      "level 59: 72\n",
      "level 60: 72\n",
      "level 61: 72\n",
      "level 62: 72\n",
      "level 63: 72\n",
      "level 64: 72\n",
      "level 65: 72\n",
      "level 66: 72\n",
      "level 67: 72\n",
      "level 68: 72\n",
      "level 69: 72\n",
      "level 70: 72\n",
      "level 71: 72\n",
      "level 72: 72\n",
      "level 73: 72\n",
      "level \n",
      "Answer: 10\n"
     ]
    }
   ],
   "source": [
    "check_index = 1\n",
    "\n",
    "print(f\"Query: {dataset[check_index]['query']}\")\n",
    "print(f\"Predicted: {responses[check_index]}\")\n",
    "print(f\"Answer: {dataset[check_index]['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
