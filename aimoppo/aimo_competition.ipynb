{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    set_seed,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import transformers\n",
    "from typing import Optional\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "max_tokens = 4096\n",
    "self_consistency_count = 17\n",
    "temperature = 0.6\n",
    "top_p = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code(code: str, remove_tmp=True, timeout=7):\n",
    "    with open(\"tmp.py\", \"w\") as f:\n",
    "        f.write(code)\n",
    "    time.sleep(0.1)\n",
    "    try:\n",
    "        output = subprocess.check_output([\"python3\", \"tmp.py\"], stderr=subprocess.STDOUT, timeout=timeout)\n",
    "        return output.decode(\"utf-8\"), True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return e.output.decode(\"utf-8\"), False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Timeout\", False\n",
    "    except Exception as e:\n",
    "        return str(e), False\n",
    "    finally:\n",
    "        if remove_tmp:\n",
    "            subprocess.run([\"rm\", \"tmp.py\"])\n",
    "            \n",
    "def get_last_python_code(text: str):\n",
    "    pattern = re.compile(r\"```python\\n(.*?)\\n```\", re.DOTALL)\n",
    "    code = \"\"\n",
    "    for match in pattern.finditer(text):\n",
    "        code = match.group(1)\n",
    "    return code\n",
    "\n",
    "def get_answer(text: str): \n",
    "    try:\n",
    "        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', text)\n",
    "        return float(result_output[0])\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype='bfloat16',\n",
    ") \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config)\n",
    "\n",
    "class StoppingCriteriaPythonCode(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return decoded_text[-3:] == \"```\" and decoded_text.count(\"```\") % 2 == 0\n",
    "    \n",
    "class StoppingCriteriaAnswer(StoppingCriteria):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pattern = re.compile(r'\\\\boxed\\{(\\d+)\\}')\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        decoded_text = decoded_text[-20:]\n",
    "        if self.pattern.search(decoded_text):\n",
    "            return True\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    set_seed(seed)\n",
    "    \n",
    "def clean_memory():\n",
    "    for _ in range(5):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(0.2)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n",
    "\\\"{}\\\"\n",
    "To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be integer, not an algebraic expression!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\n",
    "Approach:\n",
    "First, \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(prompt: str, stopping_criteria = None, past_key_values = None, return_past_key_values = False) -> str:\n",
    "    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    if len(model_inputs['input_ids'][0]) > max_tokens:\n",
    "        return prompt\n",
    "    \n",
    "    if past_key_values:\n",
    "        generation_output = model.generate(\n",
    "            **model_inputs,\n",
    "            max_length=max_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample = True,\n",
    "            #temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            num_return_sequences=1,\n",
    "            stopping_criteria = stopping_criteria,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "    else:\n",
    "        generation_output = model.generate(\n",
    "            **model_inputs,\n",
    "            max_length=max_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample = True,\n",
    "            #temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            num_return_sequences=1,\n",
    "            stopping_criteria = stopping_criteria,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    if return_past_key_values:\n",
    "        return tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True), generation_output.past_key_values\n",
    "    return tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_code(prompt: str, problem: str, return_text_output=False, max_turns=10) -> Optional[float]:\n",
    "    prompt = prompt.format(problem,\"{}\")\n",
    "    past_key_values = None\n",
    "    code_error_count = 0\n",
    "    \n",
    "    for _ in range(max_turns):\n",
    "        model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "        if model_inputs['input_ids'].shape[-1] >= max_tokens:\n",
    "            break\n",
    "        clean_memory()\n",
    "        prompt, past_key_values = model_generate(prompt, StoppingCriteriaList([StoppingCriteriaPythonCode(), StoppingCriteriaAnswer()]), past_key_values, True)\n",
    "        \n",
    "        if re.search(r'\\\\boxed\\{(\\d+)\\}', prompt[-20:]):\n",
    "            break\n",
    "        \n",
    "        code = get_last_python_code(prompt)\n",
    "        code_result, success = run_code(code)\n",
    "        if success:\n",
    "            prompt += f\"\\nCode Result: {code_result} \\n\"\n",
    "        else:\n",
    "            code_error_count += 1\n",
    "            prompt += f\"\\nCode Error: {code_result} \\nYour code has an error. Please review the problem and your code and try again.\\n\"\n",
    "            if 'is not defined' in code_result:\n",
    "                prompt += \"\\nYou need to define the variable or function that is not defined in your code.\\n\"\n",
    "            \n",
    "        if code_error_count >= 3:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    if return_text_output:\n",
    "        return get_answer(prompt), prompt\n",
    "    return get_answer(prompt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simple(prompt: str, problem: str, return_text_output=False) -> Optional[float]:\n",
    "    prompt = prompt.format(problem,\"{}\")\n",
    "    prompt = model_generate(prompt, StoppingCriteriaList([StoppingCriteriaAnswer()]))\n",
    "    if return_text_output:\n",
    "        return get_answer(prompt), prompt\n",
    "    return get_answer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dup(prompt: str, problem: str, return_text_output=False) -> Optional[float]:\n",
    "    core_question_prompt = f'{problem}\\nPlease extract the core question, only the most comprehensive and detailed one!'\n",
    "    core_question = model_generate(core_question_prompt, StoppingCriteriaList([StoppingCriteriaAnswer()]))[len(core_question_prompt):]\n",
    "    \n",
    "    \n",
    "    extract_info_prompt = f'{problem}\\nNote: Please extract the question-solving information related to the problem({core_question}), only extract the most useful information, and list them one by one!'\n",
    "    problem_solving_info = model_generate(extract_info_prompt, StoppingCriteriaList([StoppingCriteriaAnswer()]))[len(extract_info_prompt):]\n",
    "    \n",
    "    \n",
    "    extract_answer_prompt = f'{problem}\\nHint: {problem_solving_info}\\n{core_question}\\n'\n",
    "    clean_memory()\n",
    "    return predict_code(prompt, extract_answer_prompt, return_text_output)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_set = [\n",
    "    (predict_code, code),\n",
    "    (predict_dup, code)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(problem: str, self_consistency_counts=1, return_text_output = False) -> Optional[float]:\n",
    "    assert self_consistency_counts > 0\n",
    "    predicted_answer = []\n",
    "    predicted_text_output = []\n",
    "    \n",
    "    best_stats = {}\n",
    "    best_answer = None\n",
    "    best_answer_count = -1\n",
    "    \n",
    "    for i in range(self_consistency_counts):\n",
    "        logging.debug(\"=============================================Running iteration: \" + str(i))\n",
    "        if best_answer_count > np.sqrt(i):\n",
    "            logging.debug(\"Skipping iteration due to sufficient best count.\")\n",
    "            continue\n",
    "            \n",
    "        clean_memory()\n",
    "        chosen_predictor, prompt = random.choice(predictor_set)\n",
    "        result = chosen_predictor(prompt, problem, return_text_output=True)\n",
    "        logging.debug(result[1])\n",
    "        predicted_answer.append(result[0])\n",
    "        predicted_text_output.append(result[1])\n",
    "        \n",
    "        occurances = Counter(predicted_answer).most_common()\n",
    "        if occurances[0][1] > best_answer_count and occurances[0][0] is not None:\n",
    "            logging.debug(\"Found new best answer.\")\n",
    "            best_answer = occurances[0][0]\n",
    "            best_answer_count = occurances[0][1]\n",
    "        if occurances[0][1] > 5 and occurances[0][0] is not None:\n",
    "            logging.debug(\"Found sufficient occurrences of the best answer.\")\n",
    "            break\n",
    "        \n",
    "        best_stats[i] = (best_answer, best_answer_count) \n",
    "        logging.debug(occurances)\n",
    "        \n",
    "    occurances = Counter(predicted_answer).most_common(1)\n",
    "    if occurances[0][0] is not None:\n",
    "        final_answer = occurances[0][0]\n",
    "    elif len(occurances) > 1:\n",
    "        final_answer = occurances[1][0]\n",
    "    else:\n",
    "        final_answer = None\n",
    "        \n",
    "    \n",
    "    if return_text_output:\n",
    "        return final_answer, predicted_answer, predicted_text_output\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "val_dataset = Dataset.from_csv('dataset/AIMO_val.csv')\n",
    "val_dataset = val_dataset.remove_columns(['id', 'subfield', 'solution'])\n",
    "val_dataset = val_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "scores = []\n",
    "for row in tqdm(val_dataset):\n",
    "    predicted_answer = predict(row['problem'], self_consistency_count)\n",
    "    scores.append(predicted_answer == row['answer'])\n",
    "    print(\"Current Score:\", sum(scores)/len(scores))\n",
    "print(f\"Accuracy: {sum(scores)/len(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
