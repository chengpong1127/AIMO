{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_MATH_dataset, get_GSM8k_dataset\n",
    "from dataset_generator import (\n",
    "    generate_completion_dataset,\n",
    "    generate_corrective_dataset,\n",
    "    generate_kto_dataset,\n",
    "    generate_copy_dataset,\n",
    ")\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList\n",
    ")\n",
    "import re\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    ")\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"{problem} \\nPlease reason step by step, and put your final answer within \\\\boxed{{}}\"\n",
    "corrective_prompt = \"{problem}\\nHere is the incorrect solution of the problem.\\n{incorrect_solution}\\nHere is the answer got from the incorrect solution: {incorrect_answer}, Here is the correct answer of the problem.\\n{correct_answer}\\nPlease based on the incorrect solution and the correct answer, provide the correct solution of the problem by reasoning step by step and put your final answer within \\\\boxed{{}}\"\n",
    "corrective_prompt_v2 = \"\"\"\n",
    "Here is the problem: {problem}\n",
    "Here is the answer got from the incorrect solution: {incorrect_answer}\n",
    "You are a smart and talented math professor, please reason step by step, and put your final answer within \\\\boxed{{}} following the output format of the incorrect solution.\n",
    "Output format:\n",
    "\\\\boxed{{your answer}}\n",
    "\"\"\"\n",
    "dataset = get_GSM8k_dataset().select(range(2))\n",
    "\n",
    "copy_prompt = \"\"\"\n",
    "Problem: {problem}\n",
    "Correct Solution: {correct_solution}\n",
    "Incorrect Solution: {incorrect_solution}\n",
    "<End>\n",
    "This is the problem and the correct and incorrect solution generated by LLM. Please generate one similar problem with a similar correct solution and a similar incorrect solution in the same format.\n",
    "Do not generate any other unnecessary information and only generate these three parts: problem, correct solution, and incorrect solution, Put <End> at the end of the output.\n",
    "\n",
    "Problem: \"\"\"\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores):\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return decoded_text.endswith(\"<End>\")\n",
    "\n",
    "\n",
    "def extract_prompt_correct_incorrect(text):\n",
    "    print('recieved text:', text)\n",
    "    pattern = re.compile(\n",
    "        r\"\\s*(?P<problem>.+?)\\n\"\n",
    "        r\"Correct Solution:\\s*(?P<correct_solution>.+?)\\n\"\n",
    "        r\"Incorrect Solution:\\s*(?P<incorrect_solution>.+?)\\n<End>\",\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return (\n",
    "            match.group(\"problem\"),\n",
    "            match.group(\"correct_solution\"),\n",
    "            match.group(\"incorrect_solution\"),\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_answer_from_output(text):\n",
    "    try:\n",
    "        result_output = re.findall(r\"\\\\boxed\\{(\\d+)\\}\", text)\n",
    "        return float(result_output[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "generate_kargs = {\n",
    "    \"max_new_tokens\": 1000, \n",
    "    \"do_sample\": True, \n",
    "    \"batch_size\": 4,\n",
    "    \"stopping_criteria\" : StoppingCriteriaList([StoppingCriteriaSub()]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_dataset = generate_completion_dataset(\n",
    "    pipe,\n",
    "    dataset,\n",
    "    prompt,\n",
    "    get_answer_from_output,\n",
    "    generate_kwargs=generate_kargs,\n",
    "    generate_count_per_problem=1,\n",
    ")\n",
    "completion_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_dataset = completion_dataset.filter(lambda x: x[\"label\"] is False)\n",
    "corrective_dataset, history = generate_corrective_dataset(\n",
    "    incorrect_dataset,\n",
    "    corrective_prompt,\n",
    "    pipe,\n",
    "    get_answer_from_output,\n",
    "    generate_kwargs=generate_kargs,\n",
    "    corrective_solution_count_per_incorrect_solution=1,\n",
    "    return_completion_history=True,\n",
    ")\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of generate_copy_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "example_corrective_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"problem\": [\"What is the value of $\\\\frac{1}{2} + \\\\frac{1}{3}$?\"],\n",
    "        \"correct_completion\": [\n",
    "            \"$\\\\frac{1}{2} + \\\\frac{1}{3} = \\\\frac{3}{6} + \\\\frac{2}{6} = \\\\frac{5}{6}$\"\n",
    "        ],\n",
    "        \"incorrect_completion\": [\n",
    "            \"$\\\\frac{1}{2} + \\\\frac{1}{3} = \\\\frac{1+1}{2+3} = \\\\frac{2}{5}$\"\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_dataset = generate_copy_dataset(\n",
    "    example_corrective_dataset,\n",
    "    copy_prompt,\n",
    "    pipe,\n",
    "    2,\n",
    "    extract_problem_correct_incorrect=extract_prompt_correct_incorrect,\n",
    "    generate_kwargs=generate_kargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_dataset[:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
