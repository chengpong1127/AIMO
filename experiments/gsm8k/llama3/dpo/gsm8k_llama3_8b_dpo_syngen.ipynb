{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a33cbf979045f798867ed7291e86c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf7db99886044d49739c58b3aa072bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d30c74e4ed54e3788a937a4a47d53e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed745dd9b9d041f58e6bc0e0c58daaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaf177f10cf468897a4827e5d69bb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5544db2e49ad4e2c8be6de526bc1e13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f927d7abaa84ebe97448929405b2796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6d8f425ca047b1ad8c80e767a526a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Load the model and tokenizer\n",
    "class CONFIG:\n",
    "    model_name:str = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "    dataset_path: str = \"data\"\n",
    "    init_result_path: str = \"results\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG.model_name)\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(CONFIG.model_name)\n",
    "\n",
    "def llama3_8b_corrective_prompt(problem, previous_solution, correction_hint):\n",
    "    \"\"\"\n",
    "    Perform corrective prompting using Llama3 8B model.\n",
    "\n",
    "    Parameters:\n",
    "    problem (str): The problem statement.\n",
    "    previous_solution (str): The previous incorrect solution.\n",
    "    correction_hint (str): The hint or correction to guide the model.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated solution.\n",
    "    \"\"\"\n",
    "    prompt = f\"Problem: {problem}\\nPrevious Solution: {previous_solution}\\nCorrection Hint: {correction_hint}\\nNew Solution:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)  # Adjust max_length as needed\n",
    "    solution = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the generated solution after \"New Solution:\"\n",
    "    solution = solution.split(\"New Solution:\")[1].strip() if \"New Solution:\" in solution else solution.strip()\n",
    "    return solution\n",
    "\n",
    "def categorize_responses(incorrect_pairs, evaluate_function):\n",
    "    \"\"\"\n",
    "    Categorize responses into accepted and rejected after corrective prompting.\n",
    "\n",
    "    Parameters:\n",
    "    incorrect_pairs (list): List of dictionaries containing incorrect problem-solution pairs.\n",
    "    evaluate_function (callable): The function that evaluates the solution.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing JSON strings of accepted and rejected datasets.\n",
    "    \"\"\"\n",
    "    accepted = []\n",
    "    rejected = []\n",
    "\n",
    "    for item in incorrect_pairs:\n",
    "        problem = item['problem']\n",
    "        expected_solution = item['expected_solution']\n",
    "        previous_solution = item['actual_solution']\n",
    "        correction_hint = f\"The correct solution should be: {expected_solution}\"\n",
    "\n",
    "        new_solution = llama3_8b_corrective_prompt(problem, previous_solution, correction_hint)\n",
    "\n",
    "        if new_solution.strip() == expected_solution.strip():\n",
    "            accepted.append({\n",
    "                'problem': problem,\n",
    "                'expected_solution': expected_solution,\n",
    "                'previous_solution': previous_solution,\n",
    "                'new_solution': new_solution\n",
    "            })\n",
    "        else:\n",
    "            rejected.append({\n",
    "                'problem': problem,\n",
    "                'expected_solution': expected_solution,\n",
    "                'previous_solution': previous_solution,\n",
    "                'new_solution': new_solution\n",
    "            })\n",
    "\n",
    "    accepted_json = json.dumps(accepted, indent=4)\n",
    "    rejected_json = json.dumps(rejected, indent=4)\n",
    "\n",
    "    return accepted_json, rejected_json\n",
    "\n",
    "# Example usage:\n",
    "# Note: Replace the following incorrect_pairs with actual incorrect pairs data\n",
    "incorrect_pairs = [\n",
    "    {'problem': 'If a train travels 60 miles in 1 hour, how far will it travel in 4 hours?', 'expected_solution': '240 miles', 'actual_solution': '250 miles'},\n",
    "    {'problem': 'Sarah has 5 apples. She buys 7 more apples. How many apples does she have now?', 'expected_solution': '12 apples', 'actual_solution': '11 apples'},\n",
    "]\n",
    "\n",
    "accepted_json, rejected_json = categorize_responses(incorrect_pairs, llama3_8b_corrective_prompt)\n",
    "print(\"Accepted Responses JSON:\", accepted_json)\n",
    "print(\"Rejected Responses JSON:\", rejected_json)\n",
    "print(f\"Number of accepted responses: {len(json.loads(accepted_json))}\")\n",
    "print(f\"Number of rejected responses: {len(json.loads(rejected_json))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
